1
00:00:01,780 --> 00:00:03,620
Hello, and welcome back to

2
00:00:03,620 --> 00:00:06,820
another reinforcement learning
coding tutorial.

3
00:00:06,820 --> 00:00:09,260
We've come a long way since the beginning.

4
00:00:09,260 --> 00:00:11,200
For about half this series,

5
00:00:11,200 --> 00:00:14,060
you struggled through a boring
grid world environments

6
00:00:14,060 --> 00:00:16,720
as you learn the basics
of the Bellman equation,

7
00:00:16,720 --> 00:00:19,580
dynamic programming, and Q learning.

8
00:00:19,580 --> 00:00:21,200
Now, we're finally at the point

9
00:00:21,200 --> 00:00:22,340
where it's time to reveal the

10
00:00:22,340 --> 00:00:25,460
absolute cutting edge
industry standard algorithm

11
00:00:25,460 --> 00:00:26,980
which produces stellar results

12
00:00:26,980 --> 00:00:30,140
on a variety of
reinforcement learning tasks

13
00:00:30,140 --> 00:00:34,240
in both continuous, and
discrete action spaces.

14
00:00:34,240 --> 00:00:35,680
The good news is that

15
00:00:35,680 --> 00:00:37,160
it's already built on top of

16
00:00:37,160 --> 00:00:39,940
the actor critic architecture
we learned.

17
00:00:39,940 --> 00:00:42,420
So, all we need to implement it

18
00:00:42,420 --> 00:00:44,240
are a few tweaks and improvements

19
00:00:44,240 --> 00:00:45,540
to everything we've learned

20
00:00:45,540 --> 00:00:48,240
over the last few tutorials.

21
00:00:48,240 --> 00:00:50,000
If you're not an ethical person,

22
00:00:50,000 --> 00:00:51,660
please turn off this tutorial now

23
00:00:51,660 --> 00:00:55,000
because we don't want you
building robots to destroy humans.

24
00:00:55,000 --> 00:00:57,000
Do you want to destroy humans?

25
00:00:57,000 --> 00:00:57,580
Please, say no.

26
00:00:57,580 --> 00:00:59,980
Ok. I will destroy humans.

27
00:00:59,980 --> 00:01:02,480
If your ethical,
there are so many ways

28
00:01:02,480 --> 00:01:03,760
we can use this technology

29
00:01:03,760 --> 00:01:06,140
to benefit and uplift humanity.

30
00:01:07,780 --> 00:01:09,480
There are a couple main problems

31
00:01:09,480 --> 00:01:11,380
with the standard actor critic methods

32
00:01:11,380 --> 00:01:13,380
which were inhibiting results.

33
00:01:14,260 --> 00:01:17,880
First, training was extremely sensitive
to hyperparameter tuning.

34
00:01:17,880 --> 00:01:20,800
It took a lot of experimentation
to make things work.

35
00:01:20,800 --> 00:01:24,900
Second, outlier data resulting in
extreme policy updates

36
00:01:24,900 --> 00:01:28,100
could throw the entire
training process out of whack

37
00:01:28,100 --> 00:01:31,020
where bad actions lead to
inaccurate values

38
00:01:31,020 --> 00:01:34,320
and the downward spiral
becomes unrecoverable.

39
00:01:34,320 --> 00:01:36,880
You can see this
on the tensor board graph,

40
00:01:36,880 --> 00:01:40,220
if the reward just tanks.

41
00:01:40,220 --> 00:01:43,800
I ran into this myself playing
with the actor critic.

42
00:01:45,100 --> 00:01:48,960
The folks at OpenAI designed
proximal policy optimization

43
00:01:48,960 --> 00:01:50,880
with three main goals in mind.

44
00:01:50,880 --> 00:01:55,380
One, keeping the code simple
and easy to implement.

45
00:01:55,380 --> 00:01:57,860
Number two, sample efficiency.

46
00:01:58,660 --> 00:02:00,920
Policy gradients are online methods

47
00:02:00,920 --> 00:02:03,080
which means we can only train on data

48
00:02:03,080 --> 00:02:04,880
obtained from the latest policy.

49
00:02:04,880 --> 00:02:07,520
This means we have to play
a lot of episodes.

50
00:02:07,520 --> 00:02:10,820
PPO is designed to obtain
maximum efficiency

51
00:02:10,820 --> 00:02:13,300
from the training data we capture.

52
00:02:13,300 --> 00:02:17,420
Third, minimal hyper parameter tuning.

53
00:02:17,420 --> 00:02:19,420
Sane defaults just work.

54
00:02:20,420 --> 00:02:23,100
PPO includes several upgrades

55
00:02:23,100 --> 00:02:26,000
built on top of the
actor critic algorithm.

56
00:02:26,000 --> 00:02:27,760
What do they all have in common?

57
00:02:27,760 --> 00:02:31,180
They seek to maintain smooth
gradual gradient updates,

58
00:02:31,180 --> 00:02:37,040
so we get continuous improvement
and avoid unrecoverable crashes.

59
00:02:37,040 --> 00:02:38,480
The first major upgrade is called

60
00:02:38,480 --> 00:02:42,360
generalized advantage estimation,
or GAE.

61
00:02:42,360 --> 00:02:45,860
This helps smooth the discounter
rewards and reduce variance

62
00:02:45,860 --> 00:02:48,540
to make training smoother
and more stable.

63
00:02:49,620 --> 00:02:53,780
Next is using a surrogate
policy loss function.

64
00:02:53,780 --> 00:02:56,420
The policy loss function
in PPO is a ratio

65
00:02:56,420 --> 00:03:00,320
between the new probabilities
and the old probabilities

66
00:03:00,320 --> 00:03:01,900
times the advantage.

67
00:03:01,900 --> 00:03:03,900
And, we take the minimum between

68
00:03:03,900 --> 00:03:06,580
this surrogate function
and a clip version.

69
00:03:06,580 --> 00:03:10,260
This ensures a very gradual
policy update.

70
00:03:10,260 --> 00:03:13,320
The third innovation are
mini-batch updates.

71
00:03:13,320 --> 00:03:15,620
A long series of experiences sampled

72
00:03:15,620 --> 00:03:17,060
and these trajectories are

73
00:03:17,060 --> 00:03:19,620
broken into random mini batches.

74
00:03:19,620 --> 00:03:21,860
And, the network is gradually updated

75
00:03:21,860 --> 00:03:24,500
over a fixed number of epochs.

76
00:03:24,500 --> 00:03:27,900
Let's look at
each one of these in detail.

77
00:03:27,900 --> 00:03:29,620
Generalize advantage estimation

78
00:03:29,620 --> 00:03:31,340
is a way of calculating returns

79
00:03:31,340 --> 00:03:33,720
and advantages which reduces variance

80
00:03:33,720 --> 00:03:36,540
to give us smoother network updates.

81
00:03:36,540 --> 00:03:39,220
The smoothing is governed by
a hyperparameter

82
00:03:39,220 --> 00:03:41,580
called lambda between 0 and 1.

83
00:03:41,580 --> 00:03:45,160
A lambda of 1 gives
the highest accuracy.

84
00:03:45,160 --> 00:03:47,600
Lower values have greater smoothing,

85
00:03:47,600 --> 00:03:48,980
but higher bias.

86
00:03:48,980 --> 00:03:52,380
Bias being the deviance
from the correct value.

87
00:03:52,380 --> 00:03:55,240
But, less variance then
the smoother training.

88
00:03:55,240 --> 00:04:00,920
The PPO paper suggests
using a lambda of 0.95.

89
00:04:00,920 --> 00:04:04,780
Let's look at the
complete GAE algorithm.

90
00:04:04,780 --> 00:04:08,760
One, mask is 0 if the state
is terminal episode,

91
00:04:08,760 --> 00:04:10,760
or otherwise one.

92
00:04:10,760 --> 00:04:13,400
We init the GAE variable to 0,

93
00:04:13,400 --> 00:04:17,260
and loop backwards from
the last step in our data.

94
00:04:17,260 --> 00:04:20,800
We set delta to the reward plus gamma

95
00:04:20,800 --> 00:04:22,940
times the value of state prime

96
00:04:22,940 --> 00:04:24,720
times the mask.

97
00:04:24,720 --> 00:04:26,820
So, if the mask is 0,
it totally counts

98
00:04:26,820 --> 00:04:28,380
without the value of state prime

99
00:04:28,380 --> 00:04:29,920
since the episode is done.

100
00:04:29,920 --> 00:04:32,220
Minus the value of
the original state.

101
00:04:33,080 --> 00:04:37,900
Four, we update GAE as
delta plus gamma

102
00:04:37,900 --> 00:04:40,440
times lambda times a mask

103
00:04:40,440 --> 00:04:43,640
times the previous value of GAE.

104
00:04:43,640 --> 00:04:46,840
Five, the return for the state
and action is

105
00:04:46,840 --> 00:04:50,060
GAE plus the value of the state.

106
00:04:50,060 --> 00:04:53,320
So earlier, we subtracted
the value of the state

107
00:04:53,320 --> 00:04:54,440
to get the advantage.

108
00:04:54,440 --> 00:04:56,580
Now, to get the full return,
we're adding it back

109
00:04:56,580 --> 00:04:58,580
into the GAE we calculated.

110
00:04:59,180 --> 00:05:02,300
Number 6, we reverse
the list of returns

111
00:05:02,300 --> 00:05:03,940
back into the correct order,

112
00:05:03,940 --> 00:05:06,500
since we are looping backward
to start with.

113
00:05:07,760 --> 00:05:11,240
Now let's move on to
the surrogate policy loss function.

114
00:05:11,240 --> 00:05:13,040
The major difference in PPO is

115
00:05:13,040 --> 00:05:14,720
how the actor loss is calculated.

116
00:05:14,720 --> 00:05:16,460
Let's take a look.

117
00:05:16,460 --> 00:05:17,500
As stated before,

118
00:05:17,500 --> 00:05:20,620
the biggest problem with
policy gradient methods is

119
00:05:20,620 --> 00:05:22,980
ensuring gentle updates.

120
00:05:22,980 --> 00:05:25,520
The PPO loss function is based on

121
00:05:25,520 --> 00:05:29,580
the ratio of new probabilities
to old probabilities.

122
00:05:29,580 --> 00:05:32,700
Logarithmic spaces is calculated with

123
00:05:32,700 --> 00:05:37,580
new log problems minus old log probs
and then exponentiated.

124
00:05:37,580 --> 00:05:39,960
A long trajectory of experience
is collected

125
00:05:39,960 --> 00:05:41,960
prior to each update cycle.

126
00:05:41,960 --> 00:05:46,100
The old log probabilities are
saved with each action.

127
00:05:46,100 --> 00:05:47,620
With each successive update,

128
00:05:47,620 --> 00:05:51,780
we calculate the new log probability
to plug into the ratio.

129
00:05:51,780 --> 00:05:54,760
The policy actor loss in PPO is

130
00:05:54,760 --> 00:05:58,080
a minimum of two surrogate functions.

131
00:05:58,080 --> 00:06:03,240
Surrogate 1 is the ratio we calculated
times the advantage.

132
00:06:03,240 --> 00:06:05,900
Surrogate 2 is the ratio clip

133
00:06:05,900 --> 00:06:10,860
between 1 minus epsilon and
1 plus epsilon times the advantage.

134
00:06:10,860 --> 00:06:14,360
The PPO paper suggests using
an epsilon of 0.2,

135
00:06:14,360 --> 00:06:18,820
which clips the ratio
between 0.8 and 1.2.

136
00:06:18,820 --> 00:06:21,580
Value loss stays the same as
an actor critic

137
00:06:21,580 --> 00:06:22,920
the mean squared error

138
00:06:22,920 --> 00:06:25,220
between the actual calculated returns,

139
00:06:25,220 --> 00:06:27,940
and the value returned by the network.

140
00:06:27,940 --> 00:06:30,420
If you want to dive into
some technical details about

141
00:06:30,420 --> 00:06:32,420
how this works and why it works,

142
00:06:32,420 --> 00:06:35,380
as a great 20-minute video
by Arxiv Insights,

143
00:06:35,380 --> 00:06:37,600
which I will link in the description.

144
00:06:37,600 --> 00:06:41,080
Just beware the octopus slap,
and also,

145
00:06:41,080 --> 00:06:43,560
when you go to the beach,
watch out for monkeys.

146
00:06:51,760 --> 00:06:54,940
Finally, let's have a look at
the PPO update algorithm.

147
00:06:54,940 --> 00:06:58,380
Step 1, collect a batch of
n transitions,

148
00:06:58,380 --> 00:07:00,120
numbering in the thousands,

149
00:07:00,120 --> 00:07:02,360
and it should be a multiple of
the mini batch size

150
00:07:02,360 --> 00:07:04,720
from parallel environments.

151
00:07:04,720 --> 00:07:07,760
Specifically, we're going to
save the state, action,

152
00:07:07,760 --> 00:07:11,620
log probabilities of the action,
the reward obtained,

153
00:07:11,620 --> 00:07:16,240
the done mask, and the value of
the state for each step.

154
00:07:16,240 --> 00:07:19,620
Step 2, calculate the returns
for the save transitions

155
00:07:19,620 --> 00:07:22,240
using the GAE algorithm.

156
00:07:22,240 --> 00:07:25,300
Step 3, calculate the advantage,

157
00:07:25,300 --> 00:07:27,880
which is the returns
we just calculated

158
00:07:27,880 --> 00:07:30,940
minus the values of the states.

159
00:07:30,940 --> 00:07:34,360
Four, we're going to loop for e epochs.

160
00:07:34,360 --> 00:07:37,880
One, sample enough random mini batches

161
00:07:37,880 --> 00:07:41,380
from stored transitions to
cover the entire batch.

162
00:07:42,500 --> 00:07:44,640
Two, pass the state
into the network

163
00:07:44,640 --> 00:07:48,240
to get the action,
value of state prime,

164
00:07:48,240 --> 00:07:52,540
entropy, and new log probabilities
of the action.

165
00:07:52,540 --> 00:07:58,900
Three, calculate the surrogate
policy loss, and mean squared error loss.

166
00:07:58,900 --> 00:08:03,520
Four, back propagate the total loss
through the network.

167
00:08:03,520 --> 00:08:06,400
Finally, we repeat all of the above,

168
00:08:06,400 --> 00:08:08,060
until our training converges

169
00:08:08,060 --> 00:08:11,380
and we're happy with
the rewards were getting.

170
00:08:11,380 --> 00:08:14,560
All right, let's have a look
at the code.

171
00:08:15,740 --> 00:08:17,420
First, let's look at
the neural network.

172
00:08:17,420 --> 00:08:19,400
This is the exact same model we use

173
00:08:19,400 --> 00:08:21,180
with our actor critic.

174
00:08:21,180 --> 00:08:24,420
It's designed for
continuous action tasks.

175
00:08:24,420 --> 00:08:26,020
The only difference is that

176
00:08:26,020 --> 00:08:28,400
we're using the normal distribution

177
00:08:28,400 --> 00:08:29,740
to help her from PyTorch

178
00:08:29,740 --> 00:08:32,700
to sample actions from
the Gaussian distribution,

179
00:08:32,700 --> 00:08:35,020
calculate the log probabilities, etc.

180
00:08:35,020 --> 00:08:36,980
rather than doing it manually.

181
00:08:36,980 --> 00:08:38,380
The other slight difference is that

182
00:08:38,380 --> 00:08:41,500
we're using the standard deviation
as a separate parameter,

183
00:08:41,500 --> 00:08:43,840
not connected to any other layers
in the network.

184
00:08:43,840 --> 00:08:47,160
This works just as well,
but is more efficient.

185
00:08:47,160 --> 00:08:48,980
Note that, this network architecture

186
00:08:48,980 --> 00:08:50,000
could easily be adopted

187
00:08:50,000 --> 00:08:51,900
to work with a discrete action space

188
00:08:51,900 --> 00:08:54,540
by removing the
standard deviation parameter,

189
00:08:54,540 --> 00:08:56,700
and changing the
normal distribution class

190
00:08:56,700 --> 00:08:58,700
to a discrete distribution.

191
00:08:59,980 --> 00:09:00,880
All right.

192
00:09:00,880 --> 00:09:03,000
Now, let's look at the hyper parameter

193
00:09:03,000 --> 00:09:05,000
as we use in training.

194
00:09:05,000 --> 00:09:06,880
First up, NUM_ENVS.

195
00:09:06,880 --> 00:09:09,720
That's the number of environments
will run in parallel

196
00:09:09,720 --> 00:09:12,180
to generate the training data.

197
00:09:12,180 --> 00:09:13,180
HIDDEN_SIZE.

198
00:09:13,180 --> 00:09:16,980
The number of neurons in
the hidden layer of our neural network.

199
00:09:16,980 --> 00:09:18,160
LEARNING_RATE.

200
00:09:18,160 --> 00:09:20,540
This is what we passed
the atom optimizer.

201
00:09:20,540 --> 00:09:21,540
GAMMA.

202
00:09:21,540 --> 00:09:24,620
The discount factor used
to calculate returns.

203
00:09:24,620 --> 00:09:25,980
GAE_LAMBDA.

204
00:09:25,980 --> 00:09:29,380
This is the smoothing factor used
in the GAE algorithm

205
00:09:29,380 --> 00:09:31,700
which we previously discussed.

206
00:09:31,700 --> 00:09:33,140
PPO_EPSILON.

207
00:09:33,140 --> 00:09:35,140
This is used to clip the ratio

208
00:09:35,140 --> 00:09:37,140
between the new policy
and the old policy.

209
00:09:37,140 --> 00:09:41,660
A value of 0.2 will clip the ratio
between 0.8 and 1.2.

210
00:09:41,660 --> 00:09:42,960
CRITIC_DISCOUNT.

211
00:09:42,960 --> 00:09:44,980
The critic loss tends to be
quite a bit bigger

212
00:09:44,980 --> 00:09:45,840
than the actor loss,

213
00:09:45,840 --> 00:09:48,400
so we can scale it down
using this value.

214
00:09:48,400 --> 00:09:52,060
0.5 seems to work very well
and improve training.

215
00:09:52,060 --> 00:09:53,360
ENTROPY_BETA.

216
00:09:53,360 --> 00:09:55,720
The amount of importance we give
to the entropy bonus

217
00:09:55,720 --> 00:10:00,120
which improves exploration and
was discussed in previous tutorials.

218
00:10:00,120 --> 00:10:00,940
PPO_STEPS.

219
00:10:00,940 --> 00:10:02,820
The number of transitions we sample

220
00:10:02,820 --> 00:10:05,380
for each training iteration.

221
00:10:05,380 --> 00:10:07,600
Keep in mind each step
collects a transition

222
00:10:07,600 --> 00:10:09,480
from each parallel environment.

223
00:10:09,480 --> 00:10:11,160
The total amount of data collected

224
00:10:11,160 --> 00:10:14,340
will be multiplied
by NUM_ENVS from above.

225
00:10:14,340 --> 00:10:17,360
256 steps times 8 environments

226
00:10:17,360 --> 00:10:21,780
gives us a buffer of
2048 data samples to train on.

227
00:10:21,780 --> 00:10:22,780
MINI_BATCH_SIZE.

228
00:10:22,780 --> 00:10:25,100
The number of samples
that are randomly selected

229
00:10:25,100 --> 00:10:27,680
from the total amount of stored data.

230
00:10:27,680 --> 00:10:29,240
PPO_EPOCHS.

231
00:10:29,240 --> 00:10:32,780
One epoch means one pass over
the entire buffer of training data,

232
00:10:32,780 --> 00:10:36,220
so if the training buffer
has 2048 transitions

233
00:10:36,220 --> 00:10:38,520
and then mini batch sizes 64,

234
00:10:38,520 --> 00:10:42,640
one epoch would be
32 randomly selected mini batches.

235
00:10:42,640 --> 00:10:44,840
If PPO_EPOCHS is set to 10,

236
00:10:44,840 --> 00:10:46,420
will propagate the network

237
00:10:46,420 --> 00:10:48,520
over the entire buffer of
training data

238
00:10:48,520 --> 00:10:49,760
ten times.

239
00:10:49,760 --> 00:10:53,680
This will make more sense
when we get into the code below.

240
00:10:53,680 --> 00:10:54,840
TEST_EPOCHS.

241
00:10:54,840 --> 00:10:57,380
This is a count of
how often we'll run tests

242
00:10:57,380 --> 00:11:00,000
to evaluate the performance
of our network.

243
00:11:00,000 --> 00:11:03,580
Here one epoch is an entire
PPO update cycle.

244
00:11:03,580 --> 00:11:06,460
Again, this will make sense
when you see the code.

245
00:11:06,460 --> 00:11:07,640
NUM_TESTS.

246
00:11:07,640 --> 00:11:11,220
This is the number of tests we run
to average the total rewards

247
00:11:11,220 --> 00:11:14,760
each time we want to evaluate
the performance of the network.

248
00:11:14,760 --> 00:11:16,780
And finally, TARGET_REWARD.

249
00:11:16,780 --> 00:11:22,120
When we reach this reward
our evaluation will stop training.

250
00:11:22,120 --> 00:11:23,640
Now, let's jump to the main loop,

251
00:11:23,640 --> 00:11:26,860
and I'll explain other stuff
as we get to it.

252
00:11:26,860 --> 00:11:28,660
The first interesting thing is

253
00:11:28,660 --> 00:11:31,200
how we're handling
parallel environments.

254
00:11:31,200 --> 00:11:33,420
When we call envs.reset,

255
00:11:33,420 --> 00:11:35,300
it returns a list of 8 states.

256
00:11:35,300 --> 00:11:38,740
One for each parallel environment
that we created.

257
00:11:38,740 --> 00:11:41,580
When we step we pass
a list of eight actions

258
00:11:41,580 --> 00:11:43,360
and it gives us back 8 rewards,

259
00:11:43,360 --> 00:11:46,040
8 next states, and 8 dones.

260
00:11:46,760 --> 00:11:48,880
Moving down, we initialize
the network,

261
00:11:48,880 --> 00:11:51,980
the optimizer, and track
the training step.

262
00:11:51,980 --> 00:11:53,560
Train epoch which refers

263
00:11:53,560 --> 00:11:58,080
to one complete update cycle
and best reward.

264
00:11:58,080 --> 00:11:59,440
We enter the main loop

265
00:11:59,440 --> 00:12:03,160
and create empty lists
to store training data.

266
00:12:03,160 --> 00:12:05,740
Now, we loop through
the number of PPO steps.

267
00:12:05,740 --> 00:12:08,380
Keep in mind that
each PPO step generates

268
00:12:08,380 --> 00:12:12,740
a state action reward,
next state transition from each environment.

269
00:12:12,740 --> 00:12:14,860
We pass a current state
through the network

270
00:12:14,860 --> 00:12:16,880
to get the probability distribution

271
00:12:16,880 --> 00:12:19,060
and estimated value of the state.

272
00:12:19,060 --> 00:12:22,040
Also, remember that each state is
really a list of states

273
00:12:22,040 --> 00:12:23,060
one for each environment.

274
00:12:23,060 --> 00:12:25,680
Reward, done,
the distribution, and value

275
00:12:25,680 --> 00:12:29,400
all contain a batch of information
from each environment.

276
00:12:29,400 --> 00:12:31,080
We sample an action.

277
00:12:31,080 --> 00:12:33,540
Before we calculated
the log probabilities manually,

278
00:12:33,540 --> 00:12:35,000
now, we're using a helper function

279
00:12:35,000 --> 00:12:37,300
from the normal distribution class.

280
00:12:37,300 --> 00:12:40,920
We store the log probabilities,
values, rewards, done masks,

281
00:12:40,920 --> 00:12:43,440
states, and actions in a list.

282
00:12:43,440 --> 00:12:46,080
Each list will be PPO steps long,

283
00:12:46,080 --> 00:12:50,920
and each step contains
a list NUM_ENVS wide.

284
00:12:50,920 --> 00:12:53,760
In order to calculate
the returns correctly,

285
00:12:53,760 --> 00:12:58,860
we run the final next state
through the network to get its value.

286
00:12:58,860 --> 00:13:01,020
In order to calculate
the returns correctly,

287
00:13:01,020 --> 00:13:04,800
we run the final next state
through the network to get its value.

288
00:13:04,800 --> 00:13:05,900
Now, let's take a look at

289
00:13:05,900 --> 00:13:09,520
the generalized advantage
estimation function.

290
00:13:09,520 --> 00:13:11,020
Input is the next value,

291
00:13:11,020 --> 00:13:12,020
the list of rewards,

292
00:13:12,020 --> 00:13:13,140
a list of done masks,

293
00:13:13,140 --> 00:13:14,620
and the list of values.

294
00:13:14,620 --> 00:13:16,840
Keep in mind that
each step of the inputs

295
00:13:16,840 --> 00:13:20,540
contains data across
each parallel environment.

296
00:13:20,540 --> 00:13:24,940
We look backward from
the most recent experience to earlier.

297
00:13:24,940 --> 00:13:27,040
Delta is just the Bellman equation

298
00:13:27,040 --> 00:13:28,780
minus the value of the state.

299
00:13:28,780 --> 00:13:30,960
It is essentially
the same as the advantage.

300
00:13:30,960 --> 00:13:32,340
I recall from the Bellman equation

301
00:13:32,340 --> 00:13:33,980
that if the episode is over

302
00:13:33,980 --> 00:13:35,600
we use just a reward

303
00:13:35,600 --> 00:13:38,660
as a terminal state
has no next state.

304
00:13:38,660 --> 00:13:40,080
So, if an episode is done,

305
00:13:40,080 --> 00:13:42,040
multiplying by a mask of zero

306
00:13:42,040 --> 00:13:44,200
will zero out state prime.

307
00:13:44,200 --> 00:13:47,800
You can see GAE is essentially
a moving average

308
00:13:47,800 --> 00:13:49,940
of advantages discounted by

309
00:13:49,940 --> 00:13:53,200
gamma times a GAE lambda

310
00:13:53,200 --> 00:13:57,480
0.99 times 0.95
comes out to about 0.94.

311
00:13:57,480 --> 00:13:59,400
To calculate the return,

312
00:13:59,400 --> 00:14:00,980
we add the value of the state

313
00:14:00,980 --> 00:14:04,100
we previously subtracted back in.

314
00:14:04,100 --> 00:14:06,140
The return is prepend into the list

315
00:14:06,140 --> 00:14:07,860
in order to get
the correct order back,

316
00:14:07,860 --> 00:14:09,800
since we're looping in reverse.

317
00:14:09,800 --> 00:14:14,160
Again, returns is
a list PPO steps long and

318
00:14:14,160 --> 00:14:17,420
the number of environments
we're using wide.

319
00:14:17,420 --> 00:14:19,260
Jumping back down into
the main loop,

320
00:14:19,260 --> 00:14:23,040
we concatenate each list
inside a torch tensor.

321
00:14:23,040 --> 00:14:27,400
So, a list that was 256 steps long
and eight environments wide,

322
00:14:27,400 --> 00:14:30,900
simply becomes 2048 steps along.

323
00:14:30,900 --> 00:14:35,280
In the PPO paper, each step of
this process training data

324
00:14:35,280 --> 00:14:39,120
containing states, actions,
log_probs, returns, and advantages

325
00:14:39,120 --> 00:14:42,060
is referred to as a trajectory.

326
00:14:42,060 --> 00:14:43,060
Just as a side note,

327
00:14:43,060 --> 00:14:45,480
I found the number one place
I get confused,

328
00:14:45,480 --> 00:14:47,000
when I'm looking at
machine learning code is

329
00:14:47,000 --> 00:14:48,580
with tensor sizes.

330
00:14:48,580 --> 00:14:50,400
Dimensions can be confusing,

331
00:14:50,400 --> 00:14:53,220
and if they don't match
the code won't run.

332
00:14:53,220 --> 00:14:55,200
If you ever get stuck
or don't understand something,

333
00:14:55,200 --> 00:14:56,660
try printing out the tensor shapes

334
00:14:56,660 --> 00:14:58,300
before and after an operation

335
00:14:58,300 --> 00:15:00,520
to get a feel for it.

336
00:15:00,520 --> 00:15:02,320
We calculate the advantages

337
00:15:02,320 --> 00:15:04,420
by subtracting the calculator
returns from

338
00:15:04,420 --> 00:15:07,620
the network estimated values
of each state.

339
00:15:07,620 --> 00:15:09,320
We then normalize them by

340
00:15:09,320 --> 00:15:10,940
subtracting the mean of the list

341
00:15:10,940 --> 00:15:12,780
and dividing by
the standard deviation.

342
00:15:12,780 --> 00:15:15,560
This helps make training
go even smoother.

343
00:15:15,560 --> 00:15:17,380
Now, the trajectories get passed

344
00:15:17,380 --> 00:15:19,020
in the PPO update function.

345
00:15:19,020 --> 00:15:20,500
Let's take a look.

346
00:15:22,520 --> 00:15:25,880
First, some initialization code
to track statistics.

347
00:15:25,880 --> 00:15:29,460
Now, we loop over
the chosen number of PPO epochs.

348
00:15:29,460 --> 00:15:33,060
The PPO iterator function
generates random mini-batches

349
00:15:33,060 --> 00:15:35,400
covering the our update batch.

350
00:15:35,400 --> 00:15:38,600
So if the update batch
contains 2048 trajectories

351
00:15:38,600 --> 00:15:41,040
and the mini batch size is 64,

352
00:15:41,040 --> 00:15:44,920
it will generate
32 mini batches per epoch.

353
00:15:44,920 --> 00:15:46,940
We pass a state into the network

354
00:15:46,940 --> 00:15:49,280
to get the latest
probability distribution

355
00:15:49,280 --> 00:15:51,980
and value of the state.

356
00:15:51,980 --> 00:15:54,320
From this, we calculate the entropy,

357
00:15:54,320 --> 00:15:59,200
and the new log probabilities
of the originally selected actions.

358
00:15:59,200 --> 00:16:00,840
Now, we calculate the ratio of

359
00:16:00,840 --> 00:16:03,600
the new probability divided
by the old probability.

360
00:16:03,600 --> 00:16:06,180
But since we're dealing in
a logarithmic space,

361
00:16:06,180 --> 00:16:09,820
you subtract and then exponentiate.

362
00:16:09,820 --> 00:16:13,840
Surrogate 1 is the ratio
times the advantage.

363
00:16:13,840 --> 00:16:17,540
Surrogate 2 is a clipped ratio
times the advantage.

364
00:16:17,540 --> 00:16:20,240
The actor loss is the negative of

365
00:16:20,240 --> 00:16:22,720
the minimum of those two surrogates.

366
00:16:22,720 --> 00:16:25,280
The critic loss is
the mean squared error

367
00:16:25,280 --> 00:16:27,340
between the actual GAE returns

368
00:16:27,340 --> 00:16:30,740
and the network estimated
value of the state.

369
00:16:30,740 --> 00:16:35,000
The final loss is the discounted
critic loss cut in half

370
00:16:35,000 --> 00:16:36,380
plus the actor loss

371
00:16:36,380 --> 00:16:40,240
minus the entropy bonus
scaled by beta.

372
00:16:40,240 --> 00:16:43,180
We run back propagation as usual.

373
00:16:43,180 --> 00:16:48,500
We track statistics and write them
to tensor board to visualize.

374
00:16:48,500 --> 00:16:50,140
Now, back to the main code.

375
00:16:50,140 --> 00:16:51,900
One test epoch is

376
00:16:51,900 --> 00:16:55,120
one entire update operation
like you just saw.

377
00:16:55,120 --> 00:16:57,320
Every few epochs, we run
a series of tests

378
00:16:57,320 --> 00:16:58,820
and average the total rewards

379
00:16:58,820 --> 00:17:02,240
to see if our agents performance
is improving or worsening.

380
00:17:02,240 --> 00:17:03,520
The more tests you run,

381
00:17:03,520 --> 00:17:05,460
the more accurate
the statistics will be,

382
00:17:05,460 --> 00:17:08,180
but the longer it will take.

383
00:17:08,180 --> 00:17:11,000
Every time we achieve
a new best-ever reward,

384
00:17:11,000 --> 00:17:12,540
we save a check point.

385
00:17:12,540 --> 00:17:15,780
If the test reward has
exceeded our target,

386
00:17:15,780 --> 00:17:17,280
then we stop training.

387
00:17:17,280 --> 00:17:21,040
Let's take a quick look at
the test code.

388
00:17:21,040 --> 00:17:22,100
When we are training,

389
00:17:22,100 --> 00:17:24,400
we want a sample action semi-randomly

390
00:17:24,400 --> 00:17:27,980
from the probability distribution
output by the network.

391
00:17:27,980 --> 00:17:30,560
This is so we get exploration.

392
00:17:30,560 --> 00:17:32,920
But, when we were testing
the performance of our code

393
00:17:32,920 --> 00:17:35,620
most the time we want the results
to be deterministic

394
00:17:35,620 --> 00:17:37,860
and not random.

395
00:17:37,860 --> 00:17:40,280
But, when we were testing
the performance of our code,

396
00:17:40,280 --> 00:17:41,480
most of the time,

397
00:17:41,480 --> 00:17:43,620
we want the results
to be deterministic

398
00:17:43,620 --> 00:17:45,300
rather than random.

399
00:17:45,300 --> 00:17:47,280
A continuous action space
instead of

400
00:17:47,280 --> 00:17:50,400
sampling based on the mean
and standard deviation.

401
00:17:50,400 --> 00:17:52,220
We simply use the mean.

402
00:17:52,220 --> 00:17:53,660
A discrete action space,

403
00:17:53,660 --> 00:17:55,020
we would take the argmax

404
00:17:55,020 --> 00:17:57,880
or the action with
the highest probability.

405
00:17:57,880 --> 00:18:00,620
This function simply runs
through one episode,

406
00:18:00,620 --> 00:18:04,080
and returns a total reward.

407
00:18:04,080 --> 00:18:05,600
That's all the code.

408
00:18:05,600 --> 00:18:07,960
To run it, you'll need to
install Roboschool.

409
00:18:07,960 --> 00:18:09,920
Unlike MuJoCo which is commercial,

410
00:18:09,920 --> 00:18:12,540
Roboschool is completely free.

411
00:18:12,540 --> 00:18:14,860
But, it requires Mac or Linux.

412
00:18:14,860 --> 00:18:17,220
The installation process is
a bit of a pain,

413
00:18:17,220 --> 00:18:20,160
but it should work if you follow
the exact instructions

414
00:18:20,160 --> 00:18:22,200
in their Github repo.

415
00:18:22,200 --> 00:18:24,440
There's a link at the top of the code.

416
00:18:26,660 --> 00:18:28,140
Let's take a look at the results.

417
00:18:28,140 --> 00:18:30,420
I was able to get very stellar results

418
00:18:30,420 --> 00:18:32,120
on Roboschool Half Cheetah,

419
00:18:32,120 --> 00:18:35,480
and only about 40 minutes with my GPU.

420
00:18:44,720 --> 00:18:46,560
Now, it's time for some homework.

421
00:18:46,560 --> 00:18:50,780
First, I want you to play around with
other continuous environments

422
00:18:50,780 --> 00:18:55,200
either from Roboschool
or from OpenAI Gym.

423
00:18:55,200 --> 00:18:59,920
Second, try modifying this PPO code
to play Atari games

424
00:18:59,920 --> 00:19:02,060
with a discrete action space.

425
00:19:02,060 --> 00:19:03,760
It's not too complicated.

426
00:19:03,760 --> 00:19:04,720
First, you have to remove

427
00:19:04,720 --> 00:19:07,940
the standard deviation parameter
from the network.

428
00:19:07,940 --> 00:19:09,600
Second, you've got to change

429
00:19:09,600 --> 00:19:11,820
the normal distribution class
we're using

430
00:19:11,820 --> 00:19:16,020
to torch's categorical
distribution class.

431
00:19:16,020 --> 00:19:18,400
Third, in the test_env function,

432
00:19:18,400 --> 00:19:23,040
sample the argmax of probabilities
instead of the mean.

433
00:19:23,040 --> 00:19:25,280
Fourth, add the Atari rapper's

434
00:19:25,280 --> 00:19:29,200
from the other tutorials back
into them make_env function.

435
00:19:29,840 --> 00:19:31,520
Alright, post your results

436
00:19:31,520 --> 00:19:34,320
in the comments or
in our Slack channel.

437
00:19:34,320 --> 00:19:38,000
I look forward to seeing
what you can achieve with this code.

438
00:19:38,000 --> 00:19:40,740
Congratulations, you've come
a long long way

439
00:19:40,740 --> 00:19:43,060
on your reinforcement learning journey.

440
00:19:43,060 --> 00:19:45,520
You grew from basic
Bellman equation calculations,

441
00:19:45,520 --> 00:19:49,380
and grid world kindergarten
to the most advanced RL algorithm

442
00:19:49,380 --> 00:19:52,300
and production use today.

443
00:19:52,300 --> 00:19:54,600
If you've learned absolutely
nothing else from this tutorial,

444
00:19:54,600 --> 00:19:56,520
you should know one thing.

445
00:19:56,520 --> 00:19:58,560
Next time, Naughty by Nature asks you

446
00:19:58,560 --> 00:20:00,060
"If you're down with OPP?".

447
00:20:00,060 --> 00:20:01,260
You should respond,

448
00:20:01,260 --> 00:20:04,280
"No bro, I PPO".

449
00:20:05,640 --> 00:20:08,040
All right, I'm Colin Skow.

450
00:20:08,040 --> 00:20:10,320
I still have one more ace up my sleeve

451
00:20:10,320 --> 00:20:12,560
to help turn cutting edge AI research

452
00:20:12,560 --> 00:20:14,460
into code you can understand.

453
00:20:14,460 --> 00:20:17,300
I'll see you very soon
in the next tutorial.

454
00:20:17,300 --> 00:20:20,240
Meanwhile, happy coding.

