1
00:00:00,380 --> 00:00:02,580
Hi everybody, welcome back
to Arxiv insights.

2
00:00:02,580 --> 00:00:05,220
So, lately, we've seen a lot of
new emerging algorithms

3
00:00:05,220 --> 00:00:06,860
in deep reinforcement learning.

4
00:00:06,860 --> 00:00:07,800
In this episode,

5
00:00:07,800 --> 00:00:09,820
I want to dive into
one specific algorithm

6
00:00:09,820 --> 00:00:12,400
called proximal policy optimization

7
00:00:12,400 --> 00:00:14,380
that was designed at OpenAI

8
00:00:14,380 --> 00:00:17,500
and has proven successful
on a wide variety of tasks

9
00:00:17,500 --> 00:00:19,360
going all the way from robotic control

10
00:00:19,360 --> 00:00:22,920
to Atari and even playing
complicated video games

11
00:00:22,920 --> 00:00:24,260
like Dota 2.

12
00:00:24,260 --> 00:00:25,220
Now, in this episode,

13
00:00:25,220 --> 00:00:27,280
I'm going to dive into some
pretty technical terrain.

14
00:00:27,280 --> 00:00:30,180
So, I think it's good
if you're a little bit prepared.

15
00:00:30,180 --> 00:00:31,620
I've made a few previous videos

16
00:00:31,620 --> 00:00:33,680
with an introduction to
reinforcement learning

17
00:00:33,680 --> 00:00:36,080
and the problem of
the sparse reward setting.

18
00:00:36,080 --> 00:00:37,660
So, I think if you're kind of new

19
00:00:37,660 --> 00:00:39,560
to the field of
reinforcement learning,

20
00:00:39,560 --> 00:00:42,080
I would suggest to watch
those videos first,

21
00:00:42,080 --> 00:00:43,720
and then come back to this video,

22
00:00:43,720 --> 00:00:46,260
as we're going to dive
pretty deep into the rabbit hole,

23
00:00:46,260 --> 00:00:48,900
and being well-prepared
definitely as a must

24
00:00:48,900 --> 00:00:49,760
for this video.

25
00:00:49,760 --> 00:00:51,240
But, if you think you're ready for it,

26
00:00:51,240 --> 00:00:52,200
grab a cup of coffee,

27
00:00:52,200 --> 00:00:53,460
and get ready to dive in deep.

28
00:00:53,460 --> 00:00:56,740
Because, this episode is
on proximal policy optimization.

29
00:00:56,740 --> 00:00:57,740
My name is Xander.

30
00:00:57,740 --> 00:00:59,620
And welcome to our Arxiv Insights.

31
00:00:59,620 --> 00:01:01,900
[Laughter]

32
00:01:01,900 --> 00:01:09,120
[Music]

33
00:01:10,140 --> 00:01:12,500
All right, let's start by
sketching some surroundings first.

34
00:01:12,500 --> 00:01:14,080
If we're doing supervised learning

35
00:01:14,080 --> 00:01:16,140
on a data set like ImageNet,
for example,

36
00:01:16,140 --> 00:01:18,660
then we can have a
static training data set.

37
00:01:18,660 --> 00:01:20,060
We can run a stochastic

38
00:01:20,060 --> 00:01:22,280
gradient descent optimizer
in that data

39
00:01:22,280 --> 00:01:24,900
and we can be pretty sure
that our model will converge

40
00:01:24,900 --> 00:01:26,900
to a pretty decent local optimum.

41
00:01:26,900 --> 00:01:29,560
The road to success
in reinforcement learning, however,

42
00:01:29,560 --> 00:01:31,200
isn't that simple.

43
00:01:31,200 --> 00:01:33,840
One of the problems that
reinforcement learning suffers from

44
00:01:33,840 --> 00:01:35,940
is, that the training data
that is generated,

45
00:01:35,940 --> 00:01:38,760
is itself dependent
on the current policy.

46
00:01:38,760 --> 00:01:41,340
Because, our agent is
generating its own training data

47
00:01:41,340 --> 00:01:43,320
by interacting with the environment,

48
00:01:43,320 --> 00:01:45,540
rather than relying
on a static data set

49
00:01:45,540 --> 00:01:47,320
as is the case in supervised do.

50
00:01:47,320 --> 00:01:49,440
So, this means that
the data distributions

51
00:01:49,440 --> 00:01:51,160
of our observations and rewards

52
00:01:51,160 --> 00:01:54,280
are constantly changing
as our agent learns,

53
00:01:54,280 --> 00:01:56,400
which is a major cause of instability

54
00:01:56,400 --> 00:01:58,300
in the whole training process.

55
00:01:58,300 --> 00:01:59,760
An apart from having this problem

56
00:01:59,760 --> 00:02:01,900
with varying
training data distributions,

57
00:02:01,900 --> 00:02:03,740
reinforcement learning also suffers

58
00:02:03,740 --> 00:02:05,640
from a very high sensitivity

59
00:02:05,640 --> 00:02:07,180
to hyperparameter tuning

60
00:02:07,180 --> 00:02:08,920
and things like initialization,
for example.

61
00:02:08,920 --> 00:02:10,780
In some cases, it's kind of intuitive

62
00:02:10,780 --> 00:02:12,160
to understand why this happens,

63
00:02:12,160 --> 00:02:14,800
because imagine that your
learning rate is too large.

64
00:02:14,800 --> 00:02:16,760
Well, then you could have
a policy update

65
00:02:16,760 --> 00:02:18,720
that pushes your policy network

66
00:02:18,720 --> 00:02:20,860
into a region of the parameter space,

67
00:02:20,860 --> 00:02:23,380
where it's going to collect
the next batch of data

68
00:02:23,380 --> 00:02:25,220
under a very poor policy

69
00:02:25,220 --> 00:02:27,420
causing it to never recover again.

70
00:02:27,420 --> 00:02:29,820
To address many of
these annoying problems

71
00:02:29,820 --> 00:02:31,260
in reinforcement learning,

72
00:02:31,260 --> 00:02:32,980
the team and OpenAI designed a

73
00:02:32,980 --> 00:02:34,600
new reinforcement learning algorithm.

74
00:02:34,600 --> 00:02:36,980
That's called proximal
policy optimization,

75
00:02:36,980 --> 00:02:37,760
or PPO.

76
00:02:37,760 --> 00:02:40,540
The core purpose behind PPO was
to strike a balance

77
00:02:40,540 --> 00:02:43,560
between ease of implementation,
sample efficiency,

78
00:02:43,560 --> 00:02:45,040
and ease of tuning.

79
00:02:45,040 --> 00:02:47,180
Now, the first thing to
realize about PPO

80
00:02:47,180 --> 00:02:50,220
is that it is what we call
a policy gradient method.

81
00:02:50,220 --> 00:02:52,780
This means that unlike
popular key learning approaches,

82
00:02:52,780 --> 00:02:54,040
like DQN, for example,

83
00:02:54,040 --> 00:02:56,880
that can learn from
stored offline data,

84
00:02:56,880 --> 00:02:59,820
proximal policy optimization
learns online.

85
00:02:59,820 --> 00:03:01,140
This means that it doesn't use

86
00:03:01,140 --> 00:03:03,800
a replay buffer to
store past experiences,

87
00:03:03,800 --> 00:03:05,760
but instead it learns directly from

88
00:03:05,760 --> 00:03:08,640
whatever its agent encounters
in the environment.

89
00:03:08,640 --> 00:03:10,740
Once a batch of experience
has been used

90
00:03:10,740 --> 00:03:12,140
to do a gradient update,

91
00:03:12,140 --> 00:03:13,960
the experience is then discarded

92
00:03:13,960 --> 00:03:15,220
and the policy moves on.

93
00:03:15,220 --> 00:03:17,660
This also means that
policy gradient methods

94
00:03:17,660 --> 00:03:19,900
are typically less sample efficient

95
00:03:19,900 --> 00:03:21,580
than queue learning methods, because

96
00:03:21,580 --> 00:03:24,180
they only use
the collected experience once

97
00:03:24,180 --> 00:03:24,960
for doing an update.

98
00:03:24,960 --> 00:03:27,300
A general policy optimization methods

99
00:03:27,300 --> 00:03:30,200
usually start by defining
the policy gradient laws

100
00:03:30,200 --> 00:03:33,620
as the expectation over
the log of the policy actions

101
00:03:33,620 --> 00:03:36,120
times an estimate of
the advantage function.

102
00:03:36,120 --> 00:03:38,000
Okay, so what does that all mean?

103
00:03:38,000 --> 00:03:40,640
Well, the first term pi theta
is our policy.

104
00:03:40,640 --> 00:03:43,100
It's a neural network that
takes the observed states

105
00:03:43,100 --> 00:03:44,720
from the environment as an input

106
00:03:44,720 --> 00:03:47,240
and suggests actions
to take as an output.

107
00:03:47,240 --> 00:03:49,480
The second term is
the advantage function A,

108
00:03:49,480 --> 00:03:51,100
which basically tries to estimate

109
00:03:51,100 --> 00:03:54,140
what the relative value is
of the selected action

110
00:03:54,140 --> 00:03:55,220
in the current state.

111
00:03:55,220 --> 00:03:57,060
Let's take apart what that means.

112
00:03:57,060 --> 00:03:59,580
In order to compute the advantage,
we need two things.

113
00:03:59,580 --> 00:04:01,720
We need a discounted sum of rewards,

114
00:04:01,720 --> 00:04:03,600
and we need a baseline estimate.

115
00:04:03,600 --> 00:04:04,620
So, the first part is

116
00:04:04,620 --> 00:04:07,640
the discounted sum of rewards,
or the return.

117
00:04:07,640 --> 00:04:09,480
This is basically a weighted sum of

118
00:04:09,480 --> 00:04:11,320
all the rewards the agent got

119
00:04:11,320 --> 00:04:13,840
during each time step
in the current episode.

120
00:04:13,840 --> 00:04:15,460
And then, the discount factor gamma,

121
00:04:15,460 --> 00:04:19,120
which is usually somewhere
between 0.9 and 0.99,

122
00:04:19,120 --> 00:04:20,360
accounts for the fact that

123
00:04:20,360 --> 00:04:22,580
your agent cares more about reward,

124
00:04:22,580 --> 00:04:24,200
that is going to get very quickly,

125
00:04:24,200 --> 00:04:25,600
versus the same reward

126
00:04:25,600 --> 00:04:27,900
it would get
a hundred timesteps for now.

127
00:04:27,900 --> 00:04:29,580
This is exactly the same idea

128
00:04:29,580 --> 00:04:31,660
as interest in the financial world.

129
00:04:31,660 --> 00:04:33,900
In the sense that
getting money tomorrow

130
00:04:33,900 --> 00:04:35,400
is usually more valuable

131
00:04:35,400 --> 00:04:37,060
than getting
the same amount of money,

132
00:04:37,060 --> 00:04:38,700
say, a year from now.

133
00:04:38,700 --> 00:04:40,940
So, notice that
the advantage is calculated

134
00:04:40,940 --> 00:04:43,240
after the episode sequence
was collected

135
00:04:43,240 --> 00:04:44,420
from the environment.

136
00:04:44,420 --> 00:04:46,240
In other words,
we know all the rewards.

137
00:04:46,240 --> 00:04:47,640
So, there is no guessing involved

138
00:04:47,640 --> 00:04:49,420
in computing the discount or return,

139
00:04:49,420 --> 00:04:51,700
because we actually
know what happened.

140
00:04:51,700 --> 00:04:54,100
Okay, so that was the first part
of the advantage function

141
00:04:54,100 --> 00:04:56,200
the discounted sum of rewards.

142
00:04:56,200 --> 00:04:58,240
And then, the second part of
the advantage function

143
00:04:58,240 --> 00:05:00,440
is the baseline
or the value function.

144
00:05:00,440 --> 00:05:02,800
Basically, what the
value function tries to do

145
00:05:02,800 --> 00:05:05,600
is give an estimate
of the discounted sum

146
00:05:05,600 --> 00:05:07,420
of rewards from this point onward.

147
00:05:07,420 --> 00:05:08,760
Basically, it's trying to guess

148
00:05:08,760 --> 00:05:11,820
what the final return is
going to be in this episode

149
00:05:11,820 --> 00:05:13,160
starting from the current state.

150
00:05:13,160 --> 00:05:14,820
During training, this neural net

151
00:05:14,820 --> 00:05:16,940
that's representing
the value function

152
00:05:16,940 --> 00:05:18,800
is going to be frequently updated,

153
00:05:18,800 --> 00:05:20,160
using the experience that

154
00:05:20,160 --> 00:05:22,060
our agent collects
in the environment.

155
00:05:22,060 --> 00:05:24,680
Because, this is basically
a supervised learning problem.

156
00:05:24,680 --> 00:05:26,620
You're taking states as an input,

157
00:05:26,620 --> 00:05:28,720
and your neural net is
trying to predict

158
00:05:28,720 --> 00:05:31,360
what the discounted sum of
rewards is going to be

159
00:05:31,360 --> 00:05:32,580
from this state onwards.

160
00:05:32,580 --> 00:05:34,440
So, basic supervised learning.

161
00:05:34,440 --> 00:05:36,580
Notice that because
this value estimate

162
00:05:36,580 --> 00:05:38,460
is the output of a neural net,

163
00:05:38,460 --> 00:05:40,220
this is going to be a noisy estimate.

164
00:05:40,220 --> 00:05:41,500
There's going to be some variance,

165
00:05:41,500 --> 00:05:42,900
because our network is not going to

166
00:05:42,900 --> 00:05:46,120
always predict the exact value
of that states.

167
00:05:46,120 --> 00:05:48,700
So, basically, we're going to
end up with a noisy estimate

168
00:05:48,700 --> 00:05:49,720
of the value function.

169
00:05:49,720 --> 00:05:51,760
Okay, so now we have the
two terms that we need.

170
00:05:51,760 --> 00:05:54,080
We have the discounted sum
of rewards

171
00:05:54,080 --> 00:05:56,420
that we computed from
our episode rollout,

172
00:05:56,420 --> 00:05:58,060
and we have an expectation

173
00:05:58,060 --> 00:06:01,280
an estimate of that value given
the state that we're in.

174
00:06:01,280 --> 00:06:03,400
If we then subtract
the baseline estimate

175
00:06:03,400 --> 00:06:05,420
from the actual return we got,

176
00:06:05,420 --> 00:06:07,600
we get what we call
the advantage estimate.

177
00:06:07,600 --> 00:06:10,440
So, basically, the advantage estimate
is answering the question

178
00:06:10,440 --> 00:06:13,480
"how much better
was the action that I took"

179
00:06:13,480 --> 00:06:16,660
based on the expectation of
what would normally happen

180
00:06:16,660 --> 00:06:17,860
in the state that I was in.

181
00:06:17,860 --> 00:06:20,500
So, basically, was the action
that our agent took

182
00:06:20,500 --> 00:06:23,060
was it better than expected
or was it worse.

183
00:06:23,060 --> 00:06:25,640
So then, by multiplying
the log probabilities

184
00:06:25,640 --> 00:06:26,900
of your policy actions

185
00:06:26,900 --> 00:06:28,660
with this advantage function,

186
00:06:28,660 --> 00:06:31,000
we get the final
optimization objective

187
00:06:31,000 --> 00:06:33,040
that is used in policy grading.

188
00:06:33,040 --> 00:06:35,700
If you think about what
this objective function is doing,

189
00:06:35,700 --> 00:06:37,460
it's intuitively satisfying.

190
00:06:37,460 --> 00:06:40,480
Because, if the advantage
estimate was positive,

191
00:06:40,480 --> 00:06:41,800
meaning that the actions

192
00:06:41,800 --> 00:06:44,080
that the agent took
in the sample trajectory

193
00:06:44,080 --> 00:06:46,660
resulted in better than
average return,

194
00:06:46,660 --> 00:06:49,140
what we'll do is
we'll increase the probability

195
00:06:49,140 --> 00:06:51,160
of selecting them again
in the future

196
00:06:51,160 --> 00:06:52,740
when we encounter
in the same state.

197
00:06:52,740 --> 00:06:53,880
If, on the other hand,

198
00:06:53,880 --> 00:06:55,600
the advantage function
was negative,

199
00:06:55,600 --> 00:06:58,480
then we'll reduce the likelihood
of the selected actions

200
00:06:58,480 --> 00:07:00,260
which makes total sense.

201
00:07:00,260 --> 00:07:02,720
As I've already mentioned,
one of the problems is that

202
00:07:02,720 --> 00:07:04,940
if you simply keep
running gradient descent

203
00:07:04,940 --> 00:07:07,240
on one batch of collected experience,

204
00:07:07,240 --> 00:07:08,140
what will happen is that

205
00:07:08,140 --> 00:07:10,200
you'll update the parameters
in your network

206
00:07:10,200 --> 00:07:12,200
so far outside of the range,

207
00:07:12,200 --> 00:07:13,880
where this data was collected,

208
00:07:13,880 --> 00:07:15,920
that, for example,
the advantage function,

209
00:07:15,920 --> 00:07:17,480
which is, in principle,

210
00:07:17,480 --> 00:07:19,580
a noisy estimate
of the real advantage

211
00:07:19,580 --> 00:07:21,100
is going to be completely wrong.

212
00:07:21,100 --> 00:07:23,720
So, in a sense, you're just
going to destroy your policy,

213
00:07:23,720 --> 00:07:25,720
if you keep running gradient descent

214
00:07:25,720 --> 00:07:27,940
on a single batch of
collected experience.

215
00:07:27,940 --> 00:07:30,580
To solve this issue,
one successful approach

216
00:07:30,580 --> 00:07:33,040
is to make sure that
if you're updating the policy,

217
00:07:33,040 --> 00:07:36,620
you're never going to move too far away
from the old policy.

218
00:07:36,620 --> 00:07:38,380
Now, this idea was widely introduced

219
00:07:38,380 --> 00:07:41,500
in a paper called trust region
policy optimization,

220
00:07:41,500 --> 00:07:43,220
or TRPO, which is actually

221
00:07:43,220 --> 00:07:45,600
the whole basis
on which PPO was built.

222
00:07:45,600 --> 00:07:49,020
Here is the objective function
that was used in TRPO.

223
00:07:49,020 --> 00:07:50,260
If you compare this with

224
00:07:50,260 --> 00:07:53,440
the previous objective function
for vanilla policy gradients,

225
00:07:53,440 --> 00:07:54,500
what you can see is

226
00:07:54,500 --> 00:07:57,040
that the only thing
that changed in this formula

227
00:07:57,040 --> 00:07:59,180
is that the log operator is replaced

228
00:07:59,180 --> 00:08:01,620
with the division by pi theta old.

229
00:08:01,620 --> 00:08:02,840
Now, the slide here shows that

230
00:08:02,840 --> 00:08:04,940
optimizing this TRPO objective

231
00:08:04,940 --> 00:08:07,680
is, in fact, identical to
vanilla policy gradients.

232
00:08:07,680 --> 00:08:10,020
I'm not going to go into
the derivation details here.

233
00:08:10,020 --> 00:08:13,580
But, if you want, you can pause
the video or check out lecture 5

234
00:08:13,580 --> 00:08:14,980
of the deep RL bootcamp

235
00:08:14,980 --> 00:08:16,960
which will take you deep down
the rabbit hole.

236
00:08:16,960 --> 00:08:19,000
Link in the description.

237
00:08:19,000 --> 00:08:21,000
To make sure that the updated policy

238
00:08:21,000 --> 00:08:23,960
doesn't move too far away
from the current policy,

239
00:08:23,960 --> 00:08:28,520
TRPO adds a KL constraint
to the optimization objective.

240
00:08:28,520 --> 00:08:31,000
What this KL constraint
effectively does

241
00:08:31,000 --> 00:08:34,120
is it's just going to make sure
that the new updated policy

242
00:08:34,120 --> 00:08:36,420
doesn't move too far away
from the old policy.

243
00:08:36,420 --> 00:08:39,480
So, in a sense, we just want
to stick close to the region

244
00:08:39,480 --> 00:08:41,180
where we know everything works fine.

245
00:08:41,180 --> 00:08:44,460
The problem is that this KL constraint
adds additional overhead

246
00:08:44,460 --> 00:08:46,160
to our optimization process,

247
00:08:46,160 --> 00:08:49,680
and can sometimes lead to very
undesirable training behavior.

248
00:08:49,680 --> 00:08:51,640
So wouldn't it be nice,
if we can somehow

249
00:08:51,640 --> 00:08:54,760
include this extra constraint directly

250
00:08:54,760 --> 00:08:56,240
into our optimization objective.

251
00:08:56,240 --> 00:08:57,940
Well, as you might have guessed,

252
00:08:57,940 --> 00:09:00,420
that is exactly what PPO does.

253
00:09:00,420 --> 00:09:02,480
OK. So, now that we have
a little bit of surroundings,

254
00:09:02,480 --> 00:09:04,600
let's dive into the crux
of the algorithm.

255
00:09:04,600 --> 00:09:07,580
The central optimization objective
behind PPO.

256
00:09:07,580 --> 00:09:08,500
Hold on to your heads,

257
00:09:08,500 --> 00:09:09,920
it's about to get
a little technical.

258
00:09:15,540 --> 00:09:17,820
The first, let's define
a variable r theta,

259
00:09:17,820 --> 00:09:19,620
which is just a probability ratio,

260
00:09:19,620 --> 00:09:22,060
between the new updated policy outputs

261
00:09:22,060 --> 00:09:25,340
and the outputs of the previous
old version of the policy network.

262
00:09:25,340 --> 00:09:28,260
Given a sequence of
sampled actions and states,

263
00:09:28,260 --> 00:09:31,000
this r theta value
will be larger than 1,

264
00:09:31,000 --> 00:09:33,460
if the action is more likely now

265
00:09:33,460 --> 00:09:35,660
than it was in the
old version of the policy.

266
00:09:35,660 --> 00:09:38,140
It will be somewhere between 0 and 1,

267
00:09:38,140 --> 00:09:40,120
if the action is less likely now

268
00:09:40,120 --> 00:09:42,760
than it was, before
the last gradient step.

269
00:09:42,760 --> 00:09:46,680
If we multiply this ratio r theta
with the advantage function,

270
00:09:46,680 --> 00:09:50,360
we get the normal TRPO objective
in a more readable form.

271
00:09:50,360 --> 00:09:52,640
With this notation,
we can finally write down

272
00:09:52,640 --> 00:09:54,200
the central objective function

273
00:09:54,200 --> 00:09:55,740
that is used in PPO.

274
00:09:55,740 --> 00:09:57,060
Here it is.

275
00:09:57,060 --> 00:09:59,140
Looks surprisingly simple, right?

276
00:09:59,140 --> 00:10:00,180
Well, first of all,

277
00:10:00,180 --> 00:10:01,960
you can see that the objective function

278
00:10:01,960 --> 00:10:03,320
that PPO optimizes is

279
00:10:03,320 --> 00:10:05,000
an expectation operator.

280
00:10:05,000 --> 00:10:06,920
So, this means that
we're going to compute this

281
00:10:06,920 --> 00:10:08,920
over batches of trajectories.

282
00:10:08,920 --> 00:10:11,240
And, this expectation operator is taken

283
00:10:11,240 --> 00:10:13,780
over the minimum of two terms.

284
00:10:13,780 --> 00:10:16,020
The first of these terms is r theta

285
00:10:16,020 --> 00:10:17,300
times the advantage estimate.

286
00:10:17,300 --> 00:10:21,120
This is the default objective
for normal policy gradients

287
00:10:21,120 --> 00:10:23,800
which pushes the policy
towards actions

288
00:10:23,800 --> 00:10:26,360
that yield a high positive advantage

289
00:10:26,360 --> 00:10:27,460
over the baseline.

290
00:10:27,460 --> 00:10:30,300
Now, the second term is very similar
to the first one,

291
00:10:30,300 --> 00:10:32,920
except that it contains
a truncated version

292
00:10:32,920 --> 00:10:34,760
of this r theta ratio

293
00:10:34,760 --> 00:10:36,500
by applying a clipping operation

294
00:10:36,500 --> 00:10:39,360
between 1 minus epsilon
and 1 plus epsilon,

295
00:10:39,360 --> 00:10:42,880
where epsilon is usually
something like 0.2.

296
00:10:42,880 --> 00:10:46,080
Lastly, the min operator
is applied to the two terms

297
00:10:46,080 --> 00:10:47,600
to get the final result.

298
00:10:47,600 --> 00:10:50,680
While this function looks
rather simple at first sight,

299
00:10:50,680 --> 00:10:53,580
fully appreciating all
the subtleties at work here

300
00:10:53,580 --> 00:10:55,100
takes a little bit more effort.

301
00:10:55,100 --> 00:10:58,160
So bear with me here,
I promise we're almost there.

302
00:10:58,160 --> 00:10:59,680
Firstly, it's important to note

303
00:10:59,680 --> 00:11:02,780
that the advantage estimate
can be both positive and negative.

304
00:11:02,780 --> 00:11:05,760
And, this changes the effect
of the main operator.

305
00:11:05,760 --> 00:11:07,820
Here is a plot of
the objective function

306
00:11:07,820 --> 00:11:10,060
for both positive
and negative values

307
00:11:10,060 --> 00:11:12,080
of the advantage estimate.

308
00:11:12,080 --> 00:11:14,020
On the left half of the diagram,

309
00:11:14,020 --> 00:11:15,920
where the advantage function
is positive,

310
00:11:15,920 --> 00:11:18,160
or all the cases
where the selected action

311
00:11:18,160 --> 00:11:20,980
had a better-than-expected effect
on the outcome.

312
00:11:20,980 --> 00:11:22,980
And on the right half of the diagram,

313
00:11:22,980 --> 00:11:24,260
we can find situations

314
00:11:24,260 --> 00:11:27,320
where the action had
an estimated negative effect

315
00:11:27,320 --> 00:11:28,340
on the outcome.

316
00:11:28,340 --> 00:11:29,480
Now, on the left side,

317
00:11:29,480 --> 00:11:32,300
notice how the loss function
flattens out

318
00:11:32,300 --> 00:11:34,340
when r gets too high.

319
00:11:34,340 --> 00:11:36,060
This happens when the action is

320
00:11:36,060 --> 00:11:38,680
a lot more likely under
the current policy,

321
00:11:38,680 --> 00:11:41,020
than it was under the old policy.

322
00:11:41,020 --> 00:11:45,140
In this case we don't want to overdo
the action update too much,

323
00:11:45,140 --> 00:11:47,940
and so the objective function
gets clipped here

324
00:11:47,940 --> 00:11:50,580
to limit the effect
of the gradient update.

325
00:11:50,580 --> 00:11:51,920
Then, on the right side where

326
00:11:51,920 --> 00:11:54,500
the action had
an estimated negative value,

327
00:11:54,500 --> 00:11:57,120
the objective flattens
when R goes near zero.

328
00:11:57,120 --> 00:12:00,240
This corresponds to actions
that are much less likely now

329
00:12:00,240 --> 00:12:01,740
than in the old policy,

330
00:12:01,740 --> 00:12:04,520
and it will have the same effect
of not overdoing

331
00:12:04,520 --> 00:12:05,580
a similar update

332
00:12:05,580 --> 00:12:09,100
which might otherwise reduce
these action probabilities to 0.

333
00:12:09,100 --> 00:12:11,680
Remember, the advantage function
is noisy,

334
00:12:11,680 --> 00:12:15,220
so we don't want to destroy
a policy based on a single estimate.

335
00:12:15,220 --> 00:12:17,860
Finally, what about
the very right hand side?

336
00:12:17,860 --> 00:12:20,600
Well, the objective function
only ends up in this region,

337
00:12:20,600 --> 00:12:22,180
when the last gradient step

338
00:12:22,180 --> 00:12:24,660
made the selected action
a lot more probable,

339
00:12:24,660 --> 00:12:25,840
so r is big,

340
00:12:25,840 --> 00:12:28,180
while also making our policy worse,

341
00:12:28,180 --> 00:12:30,160
since the advantage is negative here.

342
00:12:30,160 --> 00:12:31,180
If that's the case,

343
00:12:31,180 --> 00:12:33,780
then we would really want to
undo the last gradient step.

344
00:12:33,780 --> 00:12:36,100
It just so happens that
the objective function

345
00:12:36,100 --> 00:12:37,960
in PPO allows us to do this.

346
00:12:37,960 --> 00:12:39,420
The function is negative here,

347
00:12:39,420 --> 00:12:42,240
so the gradient will tell us
to walk the other direction

348
00:12:42,240 --> 00:12:44,180
and make the action less probable

349
00:12:44,180 --> 00:12:45,940
by an amount proportional to

350
00:12:45,940 --> 00:12:47,920
how much we screwed it up
in the first place.

351
00:12:47,920 --> 00:12:48,800
And also, know this

352
00:12:48,800 --> 00:12:50,920
that this is the only region where

353
00:12:50,920 --> 00:12:53,460
the unclipped part of
the objective function

354
00:12:53,460 --> 00:12:55,840
has a lower value
than the clipped version,

355
00:12:55,840 --> 00:12:59,040
and those gets returned
by the minimization operator.

356
00:12:59,040 --> 00:13:00,620
Pretty clever, right?

357
00:13:00,620 --> 00:13:02,340
If you're wondering how on earth

358
00:13:02,340 --> 00:13:04,800
the authors from
the PPO paper managed to

359
00:13:04,800 --> 00:13:06,840
design this specific reward function,

360
00:13:06,840 --> 00:13:09,780
well, it's quite likely
that they had an intuitive idea

361
00:13:09,780 --> 00:13:12,000
of what they wanted
the objective function to do.

362
00:13:12,000 --> 00:13:14,520
So, they probably sketched
a bunch of diagrams

363
00:13:14,520 --> 00:13:17,400
that satisfied the behavior
that we just discussed,

364
00:13:17,400 --> 00:13:19,680
and then came up with
the exact objective function

365
00:13:19,680 --> 00:13:21,120
to make it all work out.

366
00:13:21,120 --> 00:13:22,720
Don't worry if you didn't fully get

367
00:13:22,720 --> 00:13:24,420
all the little details involved.

368
00:13:24,420 --> 00:13:26,140
Basically, the PPO objectives

369
00:13:26,140 --> 00:13:28,640
does the same as a TRP all objective,

370
00:13:28,640 --> 00:13:32,260
and that it forces the policy updates
to be conservative,

371
00:13:32,260 --> 00:13:35,060
if they move very far away
from the current policy.

372
00:13:35,060 --> 00:13:36,960
The only difference is
that PPO does this

373
00:13:36,960 --> 00:13:38,780
with a very simple objective function

374
00:13:38,780 --> 00:13:42,200
that doesn't require to calculate
all these additional constraints

375
00:13:42,200 --> 00:13:43,620
or KL divergences.

376
00:13:43,620 --> 00:13:45,040
In fact, it turns out that

377
00:13:45,040 --> 00:13:47,120
the simple PPO objective function

378
00:13:47,120 --> 00:13:50,320
often outperforms
the more complicated variant

379
00:13:50,320 --> 00:13:51,980
that we have in TRPO.

380
00:13:51,980 --> 00:13:53,840
Simplicity often wins.

381
00:13:53,840 --> 00:13:55,120
All right, nice.

382
00:13:55,120 --> 00:13:56,060
Now, that we've seen

383
00:13:56,060 --> 00:13:58,540
the central objective function
behind PPO.

384
00:13:58,540 --> 00:14:02,280
Let's take a look at
the entire algorithm end to end.

385
00:14:02,280 --> 00:14:03,560
As mentioned before,

386
00:14:03,560 --> 00:14:06,340
there are two alternating
threads in PPO.

387
00:14:06,340 --> 00:14:08,400
In the first one,
the current policy is

388
00:14:08,400 --> 00:14:10,220
interacting with the environment,

389
00:14:10,220 --> 00:14:12,320
generating episode sequences,

390
00:14:12,320 --> 00:14:15,400
for which we immediately calculate
the advantage function

391
00:14:15,400 --> 00:14:17,540
using our fitted baseline estimate

392
00:14:17,540 --> 00:14:18,880
for the state values.

393
00:14:18,880 --> 00:14:20,660
And then every so many episodes,

394
00:14:20,660 --> 00:14:21,620
a second thread

395
00:14:21,620 --> 00:14:23,660
is going to collect all that experience

396
00:14:23,660 --> 00:14:26,180
and run gradient descent
on the policy network

397
00:14:26,180 --> 00:14:28,420
using the clips PPO objective.

398
00:14:28,420 --> 00:14:31,320
As was done in training
the opening i5 system,

399
00:14:31,320 --> 00:14:34,360
these two threats can actually
be decoupled from each other

400
00:14:34,360 --> 00:14:36,580
by using thousands of remote workers

401
00:14:36,580 --> 00:14:38,400
that interact with the environment

402
00:14:38,400 --> 00:14:41,140
using a recent copy
of the policy network

403
00:14:41,140 --> 00:14:42,960
and a GPU cluster

404
00:14:42,960 --> 00:14:45,300
that runs gradient descent
on the network weights

405
00:14:45,300 --> 00:14:47,020
using the collected experience

406
00:14:47,020 --> 00:14:48,360
from those workers.

407
00:14:48,360 --> 00:14:50,340
Note that, in this case,
each worker has to

408
00:14:50,340 --> 00:14:54,040
refresh its local copy of
the policy network pretty often

409
00:14:54,040 --> 00:14:55,620
to make sure that
it's always running with

410
00:14:55,620 --> 00:14:57,660
the latest version
of the policy network

411
00:14:57,660 --> 00:14:59,860
to keep everything nicely balanced.

412
00:14:59,860 --> 00:15:02,000
Now, importantly,
the final loss function

413
00:15:02,000 --> 00:15:03,700
that is used to train an agent

414
00:15:03,700 --> 00:15:06,360
is the sum of
this clips PPO objective,

415
00:15:06,360 --> 00:15:09,360
that we just saw,
plus two additional terms

416
00:15:09,360 --> 00:15:11,400
The first additional term
of the loss function

417
00:15:11,400 --> 00:15:14,100
is basically in charge of
updating the baseline network.

418
00:15:14,100 --> 00:15:15,740
This is the part of
the network graph,

419
00:15:15,740 --> 00:15:17,300
that is in charge of estimating

420
00:15:17,300 --> 00:15:19,320
how good it is to be in this state.

421
00:15:19,320 --> 00:15:20,600
Or more specifically,

422
00:15:20,600 --> 00:15:23,620
what is the average amount of
this counted reward

423
00:15:23,620 --> 00:15:26,040
that I expect to get
from this point onwards.

424
00:15:26,040 --> 00:15:28,460
Even though the value
and policy outputs

425
00:15:28,460 --> 00:15:30,940
form two separate heads
of the same network,

426
00:15:30,940 --> 00:15:33,540
because they are part of
the same computation graph,

427
00:15:33,540 --> 00:15:35,300
you can actually combine everything

428
00:15:35,300 --> 00:15:36,840
in a single loss function

429
00:15:36,840 --> 00:15:38,840
and the auto differentiation library

430
00:15:38,840 --> 00:15:41,520
will just figure out where
to send all the gradients.

431
00:15:41,520 --> 00:15:43,540
The reason that
these two lost terms are

432
00:15:43,540 --> 00:15:45,840
actually part of
the same objective function

433
00:15:45,840 --> 00:15:48,540
is that the value estimation
network shares

434
00:15:48,540 --> 00:15:50,940
a large portion of its parameter space

435
00:15:50,940 --> 00:15:52,320
with the policy network.

436
00:15:52,320 --> 00:15:54,140
The intuition is that

437
00:15:54,140 --> 00:15:57,700
whether you're trying to estimate
the value of the current state,

438
00:15:57,700 --> 00:16:00,520
or you simply want to take
the best current action,

439
00:16:00,520 --> 00:16:02,020
you're likely going to need

440
00:16:02,020 --> 00:16:04,500
very similar feature
extraction pipelines

441
00:16:04,500 --> 00:16:06,040
from the current state observation.

442
00:16:06,040 --> 00:16:07,540
So these parts of the network

443
00:16:07,540 --> 00:16:09,020
are simply shared.

444
00:16:09,020 --> 00:16:10,600
Then, finally, the last term

445
00:16:10,600 --> 00:16:12,140
in the objective function is called

446
00:16:12,140 --> 00:16:13,360
the entropy term.

447
00:16:13,360 --> 00:16:16,560
This term is in charge of
making sure that our agent

448
00:16:16,560 --> 00:16:18,300
does enough exploration

449
00:16:18,300 --> 00:16:19,360
during training.

450
00:16:19,360 --> 00:16:22,080
In contrast to discrete
action policies

451
00:16:22,080 --> 00:16:25,120
that output the action
choice probabilities,

452
00:16:25,120 --> 00:16:26,840
the PPO policy head

453
00:16:26,840 --> 00:16:29,780
outputs the parameters
of a Gaussian distribution

454
00:16:29,780 --> 00:16:31,200
for each available action type.

455
00:16:31,200 --> 00:16:33,740
When running the agent
and training mode,

456
00:16:33,740 --> 00:16:36,460
the policy will then sample
from these distributions

457
00:16:36,460 --> 00:16:39,860
to get a continuous output value
for each action head.

458
00:16:39,860 --> 00:16:41,620
Now, if you want to fully understand

459
00:16:41,620 --> 00:16:44,360
why the entropy term
encourages exploration,

460
00:16:44,360 --> 00:16:47,400
I really recommend
to check out Aurelien Gero's video

461
00:16:47,400 --> 00:16:49,220
on the ideas behind entropy

462
00:16:49,220 --> 00:16:51,060
and KL divergence in machine learning.

463
00:16:51,060 --> 00:16:52,220
The link is in the description.

464
00:16:52,220 --> 00:16:54,940
Basically, the entropy
of a stochastic variable,

465
00:16:54,940 --> 00:16:57,880
which is driven by
an underlying probability distribution,

466
00:16:57,880 --> 00:16:59,800
is the average amount of bits

467
00:16:59,800 --> 00:17:02,120
that is needed to
represent its outcome.

468
00:17:02,120 --> 00:17:04,680
It is a measure of how unpredictable

469
00:17:04,680 --> 00:17:06,680
an outcome of this variable really is.

470
00:17:06,680 --> 00:17:08,640
So, maximizing its entropy

471
00:17:08,640 --> 00:17:12,080
will force it to have a wide spread
over all the possible options,

472
00:17:12,080 --> 00:17:14,580
resulting in the most
unpredictable outcome.

473
00:17:14,580 --> 00:17:16,460
So this gives some intuition,

474
00:17:16,460 --> 00:17:18,600
as to why adding an entropy term,

475
00:17:18,600 --> 00:17:19,940
will push the policy

476
00:17:19,940 --> 00:17:21,940
to behave a little bit more randomly

477
00:17:21,940 --> 00:17:24,140
until the other parts of the objective

478
00:17:24,140 --> 00:17:25,460
start dominating.

479
00:17:25,460 --> 00:17:27,940
As always we have a couple of
hyperparameters

480
00:17:27,940 --> 00:17:29,460
c1 and c2

481
00:17:29,460 --> 00:17:32,180
that weights the contributions
of these different parts

482
00:17:32,180 --> 00:17:33,740
in the loss function.

483
00:17:33,740 --> 00:17:35,580
For people that want to
take a deeper look

484
00:17:35,580 --> 00:17:37,940
at PPO in terms of Python code,

485
00:17:37,940 --> 00:17:41,020
I really recommend to check out
this implementation

486
00:17:41,020 --> 00:17:43,100
in PyTorch from RL-Adventure.

487
00:17:43,100 --> 00:17:45,520
Trust me. Even though you've
never worked with PyTorch,

488
00:17:45,520 --> 00:17:48,060
this implementation is
as clean as it gets.

489
00:17:48,060 --> 00:17:51,400
If you're looking for
a more production proof implementation,

490
00:17:51,400 --> 00:17:54,260
I would recommend to check out
OpenAI baselines,

491
00:17:54,260 --> 00:17:57,300
which has a full implemented
tensorflow version

492
00:17:57,300 --> 00:17:58,840
that runs on different environments

493
00:17:58,840 --> 00:18:01,000
like Atari MuJoCo and others.

494
00:18:01,000 --> 00:18:02,960
Both links are in the description.

495
00:18:02,960 --> 00:18:04,000
All right, so that's it.

496
00:18:04,000 --> 00:18:06,040
Congratulations,
if you've made it this far,

497
00:18:06,040 --> 00:18:07,620
we've covered all you need to know

498
00:18:07,620 --> 00:18:09,720
about proximal policy optimization.

499
00:18:09,720 --> 00:18:11,660
In the paper, you can find
a bunch of graphs

500
00:18:11,660 --> 00:18:15,000
that compare PPO to
other benchmarks in deep RL.

501
00:18:15,000 --> 00:18:16,640
Don't hesitate to have a look.

502
00:18:16,640 --> 00:18:18,040
The link is in the description.

503
00:18:18,040 --> 00:18:19,680
The important thing
to remember though

504
00:18:19,680 --> 00:18:22,600
is that PPO wasn't
specifically designed

505
00:18:22,600 --> 00:18:23,960
for sample efficiency,

506
00:18:23,960 --> 00:18:27,080
but rather to address
the really complicated code

507
00:18:27,080 --> 00:18:29,120
that was needed for
a lot of other algorithms.

508
00:18:29,120 --> 00:18:32,160
And, also, you know making it
relatively easy to tune

509
00:18:32,160 --> 00:18:33,760
in terms of hyperparameters.

510
00:18:33,760 --> 00:18:36,820
And, because PPO achieves
both of those objectives

511
00:18:36,820 --> 00:18:38,760
while also yielding close to

512
00:18:38,760 --> 00:18:41,180
or above state-of-the-art performance

513
00:18:41,180 --> 00:18:43,060
on a wide range of tasks,

514
00:18:43,060 --> 00:18:45,380
it has become one of the benchmarks

515
00:18:45,380 --> 00:18:47,160
in deep reinforcement learning.

516
00:18:47,160 --> 00:18:47,940
In summary,

517
00:18:47,940 --> 00:18:51,180
PPO is a state of the art policy
gradient method

518
00:18:51,180 --> 00:18:54,960
the algorithm has the stability
and reliability of TRPO

519
00:18:54,960 --> 00:18:57,180
while much simpler to implement

520
00:18:57,180 --> 00:19:01,060
requiring only a few tweaks
to vanilla policy gradient methods,

521
00:19:01,060 --> 00:19:05,000
and it can be used for a wide range
of reinforcement learning tasks.

522
00:19:05,000 --> 00:19:06,200
Great.

523
00:19:06,200 --> 00:19:07,460
Before I end this video,

524
00:19:07,460 --> 00:19:08,720
I would really like to thank

525
00:19:08,720 --> 00:19:11,360
all the people that support
this channel on patreon.

526
00:19:11,360 --> 00:19:13,640
Even if it's only $1 a month,

527
00:19:13,640 --> 00:19:15,920
those contributions really mean
a lot to me.

528
00:19:15,920 --> 00:19:17,420
They are a big motivation,

529
00:19:17,420 --> 00:19:19,500
because they show that
the people out there

530
00:19:19,500 --> 00:19:21,540
really care for the content
that I'm making

531
00:19:21,540 --> 00:19:23,440
and it's a really good motivation
to keep going.

532
00:19:23,440 --> 00:19:27,800
So, thanks a lot, all my great
amazing patreon supporters.

533
00:19:27,800 --> 00:19:30,440
That was it for this episode.

534
00:19:30,440 --> 00:19:31,960
Thank you very much for watching.

535
00:19:31,960 --> 00:19:35,360
I hope you learned something about
proximal policy optimization.

536
00:19:35,360 --> 00:19:37,980
Don't forget to like,
subscribe, and share,

537
00:19:37,980 --> 00:19:40,400
and I'd love to see you again
in the next episode

538
00:19:40,400 --> 00:19:41,980
of Arxiv Insights.

539
00:19:41,980 --> 00:19:46,540
[Music]

